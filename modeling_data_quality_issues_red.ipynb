{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import wine_red_dataset, wine_white_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "wine_red = wine_red_dataset()\n",
    "wine_red.dropna(how='all', inplace=True)\n",
    "#display(wine_red)\n",
    "\n",
    "wine_white = wine_white_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling-Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information from the exploration: Missing values in the pH (15 values) and fixed acidity (17 values) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed acidity is a missing at random value: In our case, missing fixed acidity values can be calculated from the citric acid values.     PH seem to be missing completely at random: There is no major correlation with other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed acidity missing values are calculated by a multiple imputation because the estimation can provide\n",
    "more realistic standard errors. The multiple Imputation creates multiple data\n",
    "with different estimations. The results of different models are\n",
    "averaged or combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Regression imputed fixed acidity: \n [[7. 0.]\n [8. 0.]\n [8. 0.]\n ...\n [6. 0.]\n [6. 0.]\n [6. 0.]]\n"
    }
   ],
   "source": [
    "!!!!!!!!!!NOCH BEARBEITEN!!!!!!!!!!!! --> Multiple Imputation\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "w_red_imp=wine_red[['fixed acidity','citric acid']]\n",
    "#mis_val=w_red_imp[w_red_imp.isnull().any(axis=1)]\n",
    "#display(mis_val)\n",
    "#display(mis_val.index)\n",
    "\n",
    "imp = IterativeImputer(max_iter=10)\n",
    "transformed_x= np.round(imp.fit_transform(w_red_imp))\n",
    "print(\"Regression imputed fixed acidity: \\n\",transformed_x)\n",
    "\n",
    "#df_missing_v=pd.DataFrame(transformed_x, columns=['fixed acidity','citric acid'])\n",
    "#display(df_missing_v.iloc[mis_val.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PH seem to be missing completely at random: There is no major correlation with other features, average imputation is used because it handle missling completely at randoms, ML imputation is not used because the algorithm needs a missing at random, no regression imputation is used because the exploration showed that there is no correlation with other features. The median is used for the average impuation because it's not affected by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n              missing_values=nan, strategy='mean', verbose=0)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "!!!!!!!!DIRENC NOCH DIE EINGEFÜGTEN NANS AUSGEBEN!!!!!!!!\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "x= wine_red[['pH']]\n",
    "#Average Imputation using strategy='median'\n",
    "imp_med = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_med.fit(x)\n",
    "SimpleImputer()\n",
    "#display(imp_med.transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ph raus weil die meisten eh durchscnittlich 3 irgendwas haben und geringer information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "array([5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 6., 5., 5., 6., 5.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 5., 5.,\n       6., 5., 5., 6., 6., 5., 6., 6., 5., 6., 5., 5., 5., 5., 5., 5., 5.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 6., 6., 5.,\n       5., 5., 5., 5., 5., 6., 5., 5., 5., 5., 5., 5., 5., 6., 5., 6., 5.,\n       6., 5., 5., 6., 6., 5., 5., 6., 6., 5., 5., 5., 5., 5., 5., 5., 5.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 6., 5.,\n       5., 5., 5., 5., 5., 6., 5., 5., 6., 6., 5., 5., 5., 5., 5., 5., 5.,\n       5., 5., 6., 5., 6., 5., 5., 5., 5., 6., 6., 6., 5., 5., 6., 5., 6.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 5., 6.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n       5., 5., 5., 5., 5., 6., 6., 5., 6., 5., 5., 5., 5., 6., 6., 5., 5.,\n       6., 6., 5., 5., 5., 5., 5., 6., 5., 5., 5., 5., 5., 5., 5., 5., 6.,\n       6., 5., 6., 6., 6., 5., 5., 6., 5., 5., 5., 5., 5., 5., 5., 6., 5.,\n       5., 5., 6., 5., 5., 5., 6., 6., 5., 6., 5., 5., 5., 5., 5., 5., 6.,\n       5., 5., 5., 5., 6., 6., 5., 6., 5., 6., 6., 6., 5., 5., 6., 5., 6.,\n       6., 6., 5., 6., 5., 6., 5., 5., 6., 6., 6., 5., 6., 6., 5., 6., 5.,\n       5., 5., 5., 5., 5., 6., 6., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n       5., 5., 6., 6., 5., 5., 6., 5., 6., 5., 5., 5., 5., 5., 6., 6., 5.,\n       5., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 5., 6., 5.,\n       6., 6., 6., 5., 6., 5., 5., 6., 6., 6., 6., 6., 6., 5., 5., 6., 5.,\n       6., 5., 6., 5., 5., 5., 6., 5., 5., 6., 5., 6., 6., 6., 6., 6., 6.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 5., 5., 6., 5.,\n       6., 6., 5., 5., 6., 5., 5., 5., 6., 6., 6., 6., 6., 5., 5., 6., 5.,\n       6., 5., 6., 6., 5., 6., 5., 6., 6., 5., 5., 5., 6., 5., 6., 5., 5.,\n       5., 5., 6., 5., 5., 5., 6., 5., 6., 6., 5., 6., 6., 5., 6., 6., 5.,\n       5., 6., 6., 6., 5., 5., 6., 5., 6., 5., 6., 5., 5., 6., 6., 6., 5.,\n       5., 6., 6., 6., 6., 5., 5., 6., 6., 5., 5., 6., 6., 6., 6., 6., 5.,\n       5., 5., 6., 6., 5., 6., 6., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6.,\n       6., 6., 6., 6., 5., 5., 6., 6., 5., 5., 6., 6., 5., 6., 5., 6., 6.,\n       6., 5., 5., 5., 5., 6., 6., 6., 5., 5., 6., 6., 6., 6., 5., 6., 5.,\n       6., 6., 6., 5., 6., 5., 6., 5., 5., 5., 6., 6., 5., 5., 5., 5., 5.,\n       6., 6., 6., 6., 6., 6., 6., 5., 5., 5., 6., 6., 5., 5., 6., 6., 6.,\n       6., 6., 5., 6., 6., 5., 5., 5., 6., 5., 5., 5., 5., 6., 5., 6., 5.,\n       6., 6., 5., 5., 5., 5., 5., 5., 6., 6., 5., 6., 5., 5., 5., 5., 5.,\n       5., 6., 5., 6., 5., 6., 6., 5., 5., 6., 6., 6., 5., 5., 5., 6., 5.,\n       5., 5., 5., 5., 5., 5., 6., 6., 5., 5., 5., 5., 5., 5., 6., 5., 5.,\n       5., 5., 5., 6., 5., 6., 6., 5., 5., 5., 6., 6., 5., 5., 5., 6., 5.,\n       6., 5., 5., 5., 6., 6., 5., 5., 6., 6., 6., 5., 5., 5., 5., 6., 6.,\n       6., 5., 5., 6., 5., 6., 5., 6., 5., 6., 5., 5., 5., 6., 5., 5., 5.,\n       5., 5., 6., 5., 5., 5., 6., 5., 5., 5., 5., 5., 5., 5., 6., 6., 6.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 6., 6.,\n       5., 5., 6., 5., 6., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 5.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5.,\n       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 6., 6., 5.,\n       6., 6., 6., 5., 5., 6., 5., 5., 6., 6., 6., 5., 6., 6., 6., 6., 6.,\n       6., 6., 6., 6., 5., 5., 5., 6., 5., 5., 5., 6., 6., 6., 6., 6., 6.,\n       6., 6., 6., 5., 5., 6., 6., 6., 5., 6., 5., 6., 5., 6., 5., 5., 5.,\n       5., 5., 5., 5., 5., 6., 6., 6., 6., 6., 6., 6., 5., 5., 5., 5., 5.,\n       5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 5., 5., 6.,\n       6., 5., 5., 6., 5., 6., 6., 5., 6., 5., 6., 5., 5., 6., 6., 6., 6.,\n       5., 6., 6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n       5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 6., 6., 6., 5., 5.,\n       5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6.,\n       6., 6., 6., 6., 6., 6., 6., 6., 5., 6., 5., 5., 6., 6., 6., 6., 5.,\n       6., 5., 6., 6., 6., 6., 6., 5., 5., 5., 6., 6., 6., 5., 6., 6., 6.,\n       6., 6., 5., 5., 6., 5., 5., 5., 5., 5., 5., 6., 6., 5., 6., 6., 6.,\n       6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 5., 6., 6., 6., 6., 6.,\n       6., 6., 6., 6., 6., 6., 5., 7., 6., 6., 6., 6., 6., 5., 6., 5., 6.,\n       6., 5., 6., 6., 5., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 6.,\n       6., 5., 5., 6., 5., 6., 6., 6., 6., 6., 6., 6., 5., 6., 6., 6., 6.,\n       6., 5., 5., 6., 5., 6., 6., 6., 6., 5., 6., 5., 5., 6., 5., 5., 6.,\n       6., 6., 6., 6., 6., 6., 6., 5., 6., 5., 5., 6., 5., 6., 6., 6., 6.,\n       6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n       6., 6., 6., 6., 6., 6., 6., 5., 6., 6., 5., 6., 6., 6., 6., 6., 6.,\n       5., 5., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 6., 6.,\n       5., 6., 6., 6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 5., 5.,\n       6., 5., 6., 5., 6., 6., 6., 6., 5., 5., 6., 6., 6., 5., 5., 6., 5.,\n       6., 5., 5., 5., 5., 5., 6., 5., 5., 6., 6., 5., 6., 6., 6., 5., 6.,\n       6., 6., 5., 6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 5., 6., 6., 5.,\n       5., 5., 6., 5., 6., 6., 5., 5., 6., 5., 5., 6., 5., 6., 5., 6., 6.,\n       5., 5., 6., 5., 6., 6., 6., 6., 5., 5., 5., 6., 6., 5., 5., 6., 6.,\n       5., 5., 5., 5., 6., 6., 6., 6., 5., 6., 6., 6., 6., 5., 6., 5., 6.,\n       5., 5., 6., 6., 6., 6., 5., 6., 6., 6., 6., 5., 5., 6., 6., 6., 5.,\n       6., 5., 5., 6., 6., 5., 6., 6., 6., 6., 5., 5., 5., 6., 5., 5., 5.,\n       6., 5., 5., 5., 5., 6., 6., 5., 5., 5., 6., 6., 6., 6., 6., 6., 6.,\n       5., 5., 5., 5., 5., 5., 5., 6., 5., 5., 5., 6., 6., 6., 6., 6., 6.,\n       6., 6., 5., 5., 6., 5., 6., 5., 5., 5., 5., 5., 6., 5., 6., 6., 5.,\n       6., 5., 6., 5., 5., 5., 5., 5., 5., 5., 5., 6., 5., 6., 6., 5., 5.,\n       5., 5., 5., 5., 5., 5., 5., 6., 6., 5., 5., 5., 5., 5., 5., 6., 5.,\n       5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 5., 6., 5., 6., 6.,\n       5., 5., 5., 5., 6., 6., 6., 6., 6., 5., 6., 6., 5., 5., 5., 5., 6.,\n       6., 6., 5., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 5., 6., 6., 5.,\n       5., 6., 6., 6., 5., 6., 6., 5., 5., 6., 5., 6., 5., 5., 6., 6., 6.,\n       5., 6., 5., 6., 5., 6., 5., 6., 5., 6., 6., 5., 5., 6., 6., 6., 6.,\n       6., 6., 5., 6., 6., 5., 6., 5., 6., 5., 5., 5., 6., 6., 5., 6., 6.,\n       6., 6., 6., 5., 5., 5., 5., 5., 6., 6., 6., 5., 6., 5., 6., 6., 6.,\n       5., 5., 6., 6., 6., 6., 5., 6., 5., 6., 5., 6., 6., 6., 6., 6., 6.,\n       6., 6., 5., 6., 6., 6., 6., 5., 5., 6., 5., 6., 6., 5., 6., 5., 5.,\n       5., 5., 5., 5., 5., 6., 6., 5., 5., 6., 6., 6., 5., 6., 5., 6., 6.,\n       6., 6., 6., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6.,\n       6.])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0               7.4              0.70         0.00             1.9       0.08   \n1               7.8              0.88         0.00             2.6       0.10   \n2               7.8              0.76         0.04             2.3       0.09   \n3              11.2              0.28         0.56             1.9       0.08   \n4               7.4              0.70         0.00             1.9       0.08   \n...             ...               ...          ...             ...        ...   \n1591            6.8              0.62         0.08             1.9       0.07   \n1593            5.9              0.55         0.10             2.2       0.06   \n1594            6.3              0.51         0.13             2.3       0.08   \n1595            5.9              0.65         0.12             2.0       0.08   \n1596            6.0              0.31         0.47             3.6       0.07   \n\n      flavanoids  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n0           0.53                 11.0                  34.0      1.0  3.51   \n1           0.53                 25.0                  67.0      1.0  3.20   \n2           0.53                 15.0                  54.0      1.0  3.26   \n3           0.53                 17.0                  60.0      1.0  3.16   \n4           0.53                 11.0                  34.0      1.0  3.51   \n...          ...                  ...                   ...      ...   ...   \n1591        0.53                 28.0                  38.0      1.0  3.42   \n1593        0.53                 39.0                  51.0      1.0  3.52   \n1594        0.53                 29.0                  40.0      1.0  3.42   \n1595        0.53                 32.0                  44.0      1.0  3.57   \n1596        0.53                 18.0                  42.0      1.0  3.39   \n\n      sulphates  magnesium  alcohol  lightness  quality  Prediction  \n0          0.56       0.86      9.4      0.109      5.0         5.0  \n1          0.68       0.56      9.8      0.107      5.0         5.0  \n2          0.65       0.47      9.8      0.106      5.0         5.0  \n3          0.58       0.33      9.8      0.111      6.0         5.0  \n4          0.56       0.91      9.4      0.107      5.0         5.0  \n...         ...        ...      ...        ...      ...         ...  \n1591       0.82       0.05      9.5      0.110      6.0         6.0  \n1593       0.76       0.82     11.2      0.090      6.0         6.0  \n1594       0.75       0.71     11.0      0.095      6.0         6.0  \n1595       0.71       0.33     10.2      0.104      5.0         6.0  \n1596       0.66       0.17     11.0      0.099      6.0         6.0  \n\n[1565 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>flavanoids</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>magnesium</th>\n      <th>alcohol</th>\n      <th>lightness</th>\n      <th>quality</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.08</td>\n      <td>0.53</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>1.0</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>0.86</td>\n      <td>9.4</td>\n      <td>0.109</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.10</td>\n      <td>0.53</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>1.0</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>0.56</td>\n      <td>9.8</td>\n      <td>0.107</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.09</td>\n      <td>0.53</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>1.0</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>0.47</td>\n      <td>9.8</td>\n      <td>0.106</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.08</td>\n      <td>0.53</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>1.0</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>0.33</td>\n      <td>9.8</td>\n      <td>0.111</td>\n      <td>6.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.08</td>\n      <td>0.53</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>1.0</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>0.91</td>\n      <td>9.4</td>\n      <td>0.107</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1591</th>\n      <td>6.8</td>\n      <td>0.62</td>\n      <td>0.08</td>\n      <td>1.9</td>\n      <td>0.07</td>\n      <td>0.53</td>\n      <td>28.0</td>\n      <td>38.0</td>\n      <td>1.0</td>\n      <td>3.42</td>\n      <td>0.82</td>\n      <td>0.05</td>\n      <td>9.5</td>\n      <td>0.110</td>\n      <td>6.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>1593</th>\n      <td>5.9</td>\n      <td>0.55</td>\n      <td>0.10</td>\n      <td>2.2</td>\n      <td>0.06</td>\n      <td>0.53</td>\n      <td>39.0</td>\n      <td>51.0</td>\n      <td>1.0</td>\n      <td>3.52</td>\n      <td>0.76</td>\n      <td>0.82</td>\n      <td>11.2</td>\n      <td>0.090</td>\n      <td>6.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>6.3</td>\n      <td>0.51</td>\n      <td>0.13</td>\n      <td>2.3</td>\n      <td>0.08</td>\n      <td>0.53</td>\n      <td>29.0</td>\n      <td>40.0</td>\n      <td>1.0</td>\n      <td>3.42</td>\n      <td>0.75</td>\n      <td>0.71</td>\n      <td>11.0</td>\n      <td>0.095</td>\n      <td>6.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>5.9</td>\n      <td>0.65</td>\n      <td>0.12</td>\n      <td>2.0</td>\n      <td>0.08</td>\n      <td>0.53</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>1.0</td>\n      <td>3.57</td>\n      <td>0.71</td>\n      <td>0.33</td>\n      <td>10.2</td>\n      <td>0.104</td>\n      <td>5.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>6.0</td>\n      <td>0.31</td>\n      <td>0.47</td>\n      <td>3.6</td>\n      <td>0.07</td>\n      <td>0.53</td>\n      <td>18.0</td>\n      <td>42.0</td>\n      <td>1.0</td>\n      <td>3.39</td>\n      <td>0.66</td>\n      <td>0.17</td>\n      <td>11.0</td>\n      <td>0.099</td>\n      <td>6.0</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1565 rows × 16 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#This can be achieved using the cross_val_predict object\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import svm\n",
    "\n",
    "#Perform cross-fold prediction with k=3\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "y_pred = cross_val_predict(clf, wine_red.dropna().drop('ID', axis=1).drop('quality', axis=1), wine_red.dropna()[['quality']], cv=3)\n",
    "df = pd.DataFrame(wine_red.dropna(),columns=wine_red.drop('ID', axis=1).columns)\n",
    "#display(y_pred)\n",
    "df['Prediction']=y_pred\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "58\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0       0.0\n1       0.0\n2       0.0\n3      -1.0\n4       0.0\n5       0.0\n6       0.0\n7      -2.0\n8      -2.0\n9       1.0\n10      0.0\n12      0.0\n13      1.0\n14      0.0\n15      0.0\n16     -1.0\n17      0.0\n18      1.0\n19     -1.0\n20     -1.0\n21      0.0\n23      0.0\n24     -1.0\n25      0.0\n26      0.0\n27      0.0\n28      0.0\n29     -1.0\n30      0.0\n31      0.0\n32      0.0\n33     -1.0\n34      0.0\n35     -1.0\n36      0.0\n37     -2.0\n38      1.0\n39      1.0\n40      1.0\n41      1.0\n42      0.0\n43      1.0\n44      0.0\n45      2.0\n46      0.0\n47      0.0\n48      0.0\n49      0.0\n50      0.0\n51     -1.0\n52     -1.0\n53      0.0\n54     -1.0\n55      0.0\n56      0.0\n57      0.0\n58      0.0\n59     -1.0\n60      0.0\n62     -2.0\n63      0.0\n64      0.0\n65      0.0\n66      0.0\n67      1.0\n68      1.0\n69      0.0\n70     -1.0\n71      0.0\n72      0.0\n73      1.0\n74      0.0\n75      0.0\n76      1.0\n77     -1.0\n78      0.0\n79      1.0\n80      0.0\n81      0.0\n82      0.0\n83      0.0\n84      0.0\n85      0.0\n86      0.0\n87      0.0\n88      1.0\n89      0.0\n90      0.0\n91      0.0\n92      1.0\n93      0.0\n94      1.0\n95      0.0\n96      1.0\n98      0.0\n99     -1.0\n100    -1.0\n101    -1.0\n102    -1.0\n103     0.0\n104     0.0\n105     0.0\n106     0.0\n107     0.0\n108    -1.0\n109     0.0\n110     0.0\n111     0.0\n112     0.0\n113    -1.0\n114     0.0\n115    -1.0\n116    -1.0\n117    -1.0\n118     0.0\n119    -1.0\n120     0.0\n121     0.0\n122     0.0\n123     0.0\n124     0.0\n125     0.0\n126     0.0\n127     0.0\n128    -1.0\n129     0.0\n130     0.0\n131     1.0\n132     1.0\n133    -1.0\n134    -1.0\n135     0.0\n136     0.0\n137     0.0\n138     0.0\n139     0.0\n140     0.0\n141     0.0\n142     0.0\n143     0.0\n144     0.0\n145     0.0\n146     0.0\n147     0.0\n148    -1.0\n149     0.0\n150     0.0\n151     2.0\n152     0.0\n153     0.0\n154     1.0\n155     0.0\n156     1.0\n157     0.0\n158     0.0\n159    -1.0\n160     0.0\n161     1.0\n162    -1.0\n163     0.0\n164     0.0\n165     0.0\n166     0.0\n167     1.0\n168    -1.0\n169     1.0\n170     1.0\n171    -1.0\n172    -1.0\n173     0.0\n174     0.0\n175     0.0\n176     0.0\n177    -1.0\n178     0.0\n179     0.0\n180     0.0\n181     0.0\n182     0.0\n183     0.0\n184    -1.0\n185     0.0\n186     0.0\n188     0.0\n189     0.0\n190     0.0\n191    -1.0\n192     0.0\n193     0.0\n194     0.0\n195     0.0\n196     0.0\n197     0.0\n198    -1.0\n199     1.0\n200    -1.0\n201     0.0\n202     0.0\n203     0.0\n204    -1.0\n205    -1.0\n206    -1.0\n207     0.0\n208     0.0\n209    -1.0\n210     0.0\n211    -1.0\n212    -1.0\n213     0.0\n214    -1.0\n215     0.0\n216     1.0\n217     0.0\n218     0.0\n219     0.0\n220    -1.0\n221     0.0\n222     0.0\n223    -1.0\n224     1.0\n225     0.0\n226     0.0\n227     0.0\n228     0.0\n229     1.0\n230    -1.0\n231    -1.0\n232    -1.0\n233     1.0\n234    -1.0\n235    -1.0\n236    -1.0\n237    -1.0\n238    -1.0\n239    -1.0\n240     0.0\n241     0.0\n242    -1.0\n243    -2.0\n244    -2.0\n245     0.0\n246     0.0\n247     0.0\n248    -1.0\n249     0.0\n250     0.0\n251    -1.0\n252     1.0\n253     0.0\n254    -1.0\n255     0.0\n256     0.0\n257     0.0\n258     0.0\n259    -1.0\n260     0.0\n261     1.0\n262     0.0\n263     0.0\n264     1.0\n265    -1.0\n266     1.0\n267    -2.0\n268    -1.0\n269     0.0\n270     0.0\n271     0.0\n272     0.0\n273     0.0\n275     0.0\n276    -1.0\n277     0.0\n278    -2.0\n279    -1.0\n280    -1.0\n281    -1.0\n282     0.0\n283    -1.0\n284     0.0\n285     0.0\n286     0.0\n287     0.0\n288    -1.0\n289     0.0\n290    -1.0\n291     1.0\n292    -1.0\n293     0.0\n294    -1.0\n295     0.0\n296     0.0\n297     0.0\n298     0.0\n299     0.0\n300     0.0\n301     0.0\n302     0.0\n303     0.0\n304     0.0\n305    -1.0\n306     0.0\n307    -1.0\n308    -1.0\n309    -1.0\n310    -1.0\n311    -1.0\n312    -1.0\n313     0.0\n314     1.0\n315     0.0\n316     0.0\n317    -1.0\n318    -1.0\n319    -1.0\n320    -1.0\n321     0.0\n322     0.0\n323    -1.0\n324    -1.0\n325    -1.0\n326    -1.0\n327     1.0\n328    -1.0\n329     0.0\n330     0.0\n331     0.0\n332    -1.0\n333     1.0\n334    -1.0\n335    -1.0\n336     0.0\n337     1.0\n338     0.0\n339    -1.0\n340     0.0\n341     0.0\n342    -1.0\n343    -1.0\n344     0.0\n345     0.0\n346    -1.0\n347     0.0\n348     0.0\n349    -1.0\n350     0.0\n351    -1.0\n352     0.0\n353     1.0\n354     0.0\n355     0.0\n356     1.0\n357    -1.0\n358    -1.0\n359    -1.0\n360     0.0\n361     0.0\n362     0.0\n363     1.0\n364    -2.0\n365     0.0\n366    -2.0\n367     0.0\n368     0.0\n369    -1.0\n370     0.0\n371    -1.0\n372     0.0\n373     0.0\n374     0.0\n375    -1.0\n376     0.0\n377    -1.0\n378     0.0\n379     0.0\n380    -1.0\n381    -1.0\n382    -1.0\n383    -1.0\n384     0.0\n385    -1.0\n386    -1.0\n387    -1.0\n388    -1.0\n389    -2.0\n390    -2.0\n391    -1.0\n392     0.0\n393     0.0\n394     0.0\n395    -1.0\n396     0.0\n397     0.0\n398     0.0\n399     0.0\n400     0.0\n401     0.0\n402    -1.0\n403    -1.0\n404     0.0\n405     0.0\n406     0.0\n407    -1.0\n408     0.0\n413    -1.0\n414     0.0\n415     0.0\n416     0.0\n417     0.0\n418     0.0\n419     0.0\n420    -1.0\n421    -1.0\n422     0.0\n423    -1.0\n424     0.0\n425    -1.0\n426     0.0\n427    -1.0\n428     0.0\n429    -1.0\n430    -1.0\n431     0.0\n432     0.0\n433     0.0\n434    -1.0\n435     0.0\n436    -1.0\n437     0.0\n438    -1.0\n439     0.0\n440    -3.0\n441     0.0\n442    -2.0\n443    -1.0\n444    -1.0\n445    -1.0\n446     1.0\n447     1.0\n448    -1.0\n449     0.0\n450     0.0\n451    -1.0\n452    -1.0\n453    -1.0\n454     1.0\n455    -2.0\n456     0.0\n457     0.0\n458    -1.0\n459     2.0\n460     0.0\n461     0.0\n462     1.0\n463     0.0\n464    -1.0\n465     1.0\n466     0.0\n467     0.0\n468    -1.0\n469     0.0\n470     1.0\n471     0.0\n472     0.0\n473     1.0\n474    -1.0\n475     0.0\n476     1.0\n477     0.0\n478     0.0\n479    -1.0\n480     1.0\n481    -2.0\n482     1.0\n483     1.0\n484     0.0\n485     0.0\n486     0.0\n487    -1.0\n488    -1.0\n489     0.0\n490    -1.0\n491    -1.0\n492    -1.0\n493     0.0\n494     0.0\n495    -2.0\n496    -1.0\n497     1.0\n498    -2.0\n499     0.0\n500    -1.0\n501    -1.0\n502    -1.0\n503    -1.0\n504    -1.0\n505    -1.0\n506    -1.0\n507    -1.0\n508    -1.0\n509    -1.0\n510     1.0\n511    -1.0\n512    -1.0\n513    -1.0\n514    -1.0\n515     0.0\n516     0.0\n517     2.0\n518     0.0\n519     1.0\n520     0.0\n521     0.0\n522     0.0\n523     0.0\n524     0.0\n525     1.0\n526     1.0\n527     0.0\n528    -1.0\n529     0.0\n530     0.0\n531     1.0\n532     1.0\n533     0.0\n534    -1.0\n535     0.0\n536     0.0\n537     0.0\n538    -1.0\n539     1.0\n540     0.0\n541     0.0\n542     0.0\n543     0.0\n544    -1.0\n545     0.0\n546    -1.0\n547     0.0\n548     0.0\n549    -1.0\n550    -1.0\n551    -1.0\n552    -1.0\n553     0.0\n554     1.0\n555     1.0\n556     0.0\n557     1.0\n558     0.0\n559     0.0\n560     1.0\n561     0.0\n562     0.0\n563    -1.0\n564     0.0\n565     1.0\n566    -1.0\n567    -1.0\n568     0.0\n569     0.0\n570     0.0\n571     0.0\n572     1.0\n573     1.0\n574     0.0\n575     0.0\n576     1.0\n577     0.0\n578     0.0\n579     0.0\n580     0.0\n581     0.0\n582     0.0\n583    -2.0\n584    -1.0\n585    -1.0\n586    -1.0\n587     0.0\n588    -2.0\n589    -1.0\n590     0.0\n591    -1.0\n592     0.0\n593     0.0\n594     0.0\n595     0.0\n596     0.0\n597     0.0\n598    -1.0\n599     0.0\n600     1.0\n601    -1.0\n602     0.0\n603    -1.0\n604    -1.0\n605    -1.0\n609     0.0\n610     0.0\n611     1.0\n612    -1.0\n613     1.0\n614     0.0\n615     0.0\n616     0.0\n617     0.0\n618     1.0\n619     1.0\n620     0.0\n621     0.0\n622     0.0\n623     0.0\n624     0.0\n625     0.0\n626     0.0\n627     0.0\n628    -1.0\n629     0.0\n630    -1.0\n631     1.0\n632     0.0\n633     1.0\n634     0.0\n635     0.0\n636     0.0\n637     0.0\n638    -2.0\n639     0.0\n640     0.0\n641     0.0\n642     0.0\n643     0.0\n644     0.0\n645    -1.0\n646     0.0\n647     2.0\n648    -1.0\n649    -1.0\n650     0.0\n651     0.0\n652     1.0\n653     0.0\n654     0.0\n655     0.0\n656     0.0\n657    -1.0\n658    -1.0\n659     2.0\n660    -1.0\n661     0.0\n662    -1.0\n663     0.0\n664     1.0\n665     0.0\n666    -1.0\n667     0.0\n668     1.0\n669     0.0\n670     0.0\n671     0.0\n672     0.0\n673     0.0\n674     0.0\n675     1.0\n676     0.0\n677     0.0\n678     0.0\n679     1.0\n680     0.0\n681     0.0\n682     0.0\n683     1.0\n684     0.0\n685     1.0\n686     0.0\n687     0.0\n688     0.0\n689     1.0\n690     2.0\n691     0.0\n692     0.0\n693     0.0\n694     0.0\n695     0.0\n696    -1.0\n697    -1.0\n698     0.0\n699     0.0\n700    -1.0\n701    -1.0\n702    -1.0\n703     1.0\n704     1.0\n705     0.0\n706     0.0\n707     1.0\n708     0.0\n709     0.0\n710     0.0\n711     0.0\n712     0.0\n713     0.0\n714     0.0\n715    -1.0\n716     0.0\n717     0.0\n718     0.0\n719     0.0\n720     0.0\n721     0.0\n722     0.0\n723     1.0\n724     1.0\n725     1.0\n726     0.0\n727     0.0\n728     0.0\n729     0.0\n730     0.0\n731     1.0\n732     0.0\n733     0.0\n734     0.0\n735     0.0\n736     0.0\n737    -1.0\n738     0.0\n739     0.0\n740     0.0\n741     0.0\n742     0.0\n743     0.0\n744     0.0\n745    -1.0\n746    -1.0\n747     0.0\n748    -1.0\n749    -1.0\n750     0.0\n751     0.0\n752     0.0\n754    -1.0\n755    -1.0\n756    -1.0\n757     0.0\n758     0.0\n760     0.0\n761     0.0\n762    -1.0\n763     0.0\n764    -1.0\n765    -1.0\n766     0.0\n767     0.0\n768    -1.0\n769     0.0\n770    -1.0\n771     0.0\n772     0.0\n773    -1.0\n774    -1.0\n775     0.0\n776    -1.0\n777    -1.0\n778     1.0\n779     0.0\n780    -1.0\n781     0.0\n782     0.0\n783     0.0\n784     0.0\n785     0.0\n786     0.0\n787    -1.0\n788    -1.0\n789     0.0\n790    -1.0\n791     0.0\n792    -1.0\n793     1.0\n794     0.0\n795     1.0\n796     0.0\n797    -1.0\n798     0.0\n799     0.0\n800     0.0\n801     0.0\n802    -1.0\n803    -1.0\n804    -1.0\n805    -1.0\n806    -1.0\n807    -1.0\n808     0.0\n809     0.0\n810     1.0\n811     0.0\n812     1.0\n813     2.0\n814     0.0\n815     1.0\n816     0.0\n817     0.0\n818     0.0\n819     0.0\n820     0.0\n821    -1.0\n822     0.0\n823     0.0\n824     0.0\n825     1.0\n826    -1.0\n827     1.0\n828    -2.0\n829     0.0\n830     2.0\n831     0.0\n832     3.0\n833     2.0\n834     0.0\n835     0.0\n836    -1.0\n837    -1.0\n838    -1.0\n839     0.0\n840    -1.0\n841     0.0\n842     0.0\n843     0.0\n844     0.0\n845     0.0\n846     0.0\n847    -1.0\n848     0.0\n849     0.0\n850     0.0\n851     0.0\n852     0.0\n853     0.0\n854     0.0\n855    -1.0\n856     0.0\n857    -1.0\n858    -1.0\n859     0.0\n860     0.0\n861    -1.0\n862     0.0\n863     0.0\n864     0.0\n865     0.0\n866     0.0\n867     0.0\n868     0.0\n869     0.0\n870     0.0\n871     1.0\n872     2.0\n873    -1.0\n874    -1.0\n875    -1.0\n876     2.0\n877     0.0\n878    -1.0\n879     0.0\n880     0.0\n881     0.0\n882     0.0\n883     0.0\n884    -1.0\n885     1.0\n886    -1.0\n887    -1.0\n888     0.0\n889     0.0\n890     1.0\n891     0.0\n892     0.0\n893     0.0\n894    -1.0\n895     0.0\n896    -1.0\n897     0.0\n898    -1.0\n899     2.0\n900     1.0\n901    -1.0\n902    -1.0\n903    -1.0\n904    -1.0\n905     0.0\n906     1.0\n907     0.0\n908     0.0\n909     0.0\n910     0.0\n911     0.0\n912     0.0\n913    -1.0\n914     0.0\n915     0.0\n916     0.0\n917     0.0\n918     0.0\n919     0.0\n920     1.0\n921     0.0\n922     0.0\n923     0.0\n924     1.0\n925    -1.0\n926     0.0\n927     1.0\n928     1.0\n929    -1.0\n930     1.0\n931     0.0\n932    -1.0\n933     0.0\n934     1.0\n935     0.0\n936     0.0\n937     2.0\n938    -1.0\n939     1.0\n940    -1.0\n941    -1.0\n942    -1.0\n943    -2.0\n944    -1.0\n945    -1.0\n946    -1.0\n947    -1.0\n949    -1.0\n950    -1.0\n951    -1.0\n952    -1.0\n953    -1.0\n954     0.0\n955     1.0\n956     0.0\n957     0.0\n958    -1.0\n959     0.0\n960     0.0\n961     0.0\n962     0.0\n963     0.0\n964     0.0\n965     0.0\n966    -1.0\n967     0.0\n968     0.0\n969     0.0\n970     0.0\n971     0.0\n972    -1.0\n973     1.0\n974    -1.0\n975     0.0\n976     0.0\n977     0.0\n978    -1.0\n979     1.0\n980     0.0\n981     0.0\n982     0.0\n983     0.0\n984     1.0\n985     0.0\n986    -1.0\n987     0.0\n988     0.0\n989     0.0\n990     0.0\n991     0.0\n992    -1.0\n993     0.0\n994     0.0\n995    -1.0\n996    -1.0\n997    -1.0\n998    -1.0\n999     0.0\n1000   -1.0\n1001   -1.0\n1002   -1.0\n1003   -1.0\n1004    1.0\n1005   -1.0\n1006   -1.0\n1007   -1.0\n1008   -1.0\n1009    1.0\n1010   -1.0\n1011    0.0\n1012    0.0\n1013   -1.0\n1014    0.0\n1015    0.0\n1016   -1.0\n1017    0.0\n1018    0.0\n1019    1.0\n1020    0.0\n1021    0.0\n1022    1.0\n1023    0.0\n1024   -1.0\n1025   -1.0\n1026    1.0\n1027    1.0\n1028    0.0\n1029   -1.0\n1030   -1.0\n1031   -1.0\n1032    0.0\n1033    0.0\n1034   -1.0\n1035   -1.0\n1036   -1.0\n1037    0.0\n1038   -1.0\n1039    0.0\n1040    0.0\n1041   -1.0\n1042    0.0\n1043   -1.0\n1044    0.0\n1045    0.0\n1046    0.0\n1047    1.0\n1048    0.0\n1049    0.0\n1050    1.0\n1051    0.0\n1052    1.0\n1053   -1.0\n1054   -1.0\n1055   -1.0\n1056   -1.0\n1057    0.0\n1058   -1.0\n1059   -1.0\n1060    0.0\n1061   -2.0\n1062    0.0\n1063    0.0\n1064    0.0\n1065   -1.0\n1066   -1.0\n1067   -1.0\n1068   -1.0\n1069    1.0\n1070   -1.0\n1071    0.0\n1072   -1.0\n1073    0.0\n1074    0.0\n1075   -1.0\n1076    0.0\n1077    1.0\n1078    1.0\n1079   -2.0\n1080    0.0\n1081   -2.0\n1082   -1.0\n1083    0.0\n1084   -1.0\n1085    0.0\n1086   -1.0\n1087    0.0\n1088   -1.0\n1089   -1.0\n1090   -2.0\n1091    0.0\n1092    0.0\n1093   -1.0\n1094   -1.0\n1095    1.0\n1096   -1.0\n1097    0.0\n1098   -1.0\n1099    0.0\n1100    0.0\n1101    0.0\n1102    0.0\n1103    0.0\n1104    0.0\n1105    1.0\n1106    0.0\n1107   -1.0\n1108    0.0\n1109    0.0\n1110    0.0\n1111   -1.0\n1112    0.0\n1113    0.0\n1114    0.0\n1115    0.0\n1116    0.0\n1117    0.0\n1118    0.0\n1119    1.0\n1120   -2.0\n1121    0.0\n1122    0.0\n1123    0.0\n1124    2.0\n1125   -1.0\n1126    0.0\n1127    0.0\n1128    0.0\n1129    0.0\n1130    0.0\n1131    0.0\n1132   -1.0\n1133   -1.0\n1134   -1.0\n1135    0.0\n1136    0.0\n1137    0.0\n1138    0.0\n1139   -1.0\n1140   -1.0\n1141    0.0\n1142    0.0\n1143    0.0\n1144    1.0\n1145    0.0\n1146    0.0\n1147   -1.0\n1148    0.0\n1149    0.0\n1150   -1.0\n1151    0.0\n1152    0.0\n1153    0.0\n1154    0.0\n1155    0.0\n1156   -1.0\n1157   -1.0\n1158    0.0\n1159    1.0\n1160   -1.0\n1161    0.0\n1165    0.0\n1166    1.0\n1167   -1.0\n1168    0.0\n1169    0.0\n1170    0.0\n1171    0.0\n1172    0.0\n1173   -1.0\n1174   -1.0\n1175    0.0\n1176    1.0\n1177   -1.0\n1178    0.0\n1179    0.0\n1180    0.0\n1181    1.0\n1182    0.0\n1183    0.0\n1184    0.0\n1185    0.0\n1186    1.0\n1187    0.0\n1188    0.0\n1189    1.0\n1190    0.0\n1191    0.0\n1192   -1.0\n1193    0.0\n1194   -1.0\n1195   -1.0\n1196   -1.0\n1197   -1.0\n1198    0.0\n1199   -1.0\n1200   -1.0\n1201   -1.0\n1202   -2.0\n1203    0.0\n1204   -1.0\n1205   -1.0\n1206   -1.0\n1207    0.0\n1208   -1.0\n1209   -1.0\n1210    0.0\n1211    0.0\n1212    0.0\n1213    0.0\n1214    0.0\n1215    0.0\n1216   -1.0\n1217    0.0\n1218    0.0\n1219    0.0\n1220    0.0\n1221    0.0\n1222   -1.0\n1223    0.0\n1224    0.0\n1225    0.0\n1226    0.0\n1227    0.0\n1228   -1.0\n1229    0.0\n1230    0.0\n1231    1.0\n1232    0.0\n1233    1.0\n1234    0.0\n1235    1.0\n1236   -1.0\n1237    0.0\n1238    1.0\n1239    2.0\n1240    0.0\n1241    1.0\n1242    0.0\n1243    0.0\n1244   -1.0\n1245    1.0\n1246    0.0\n1247    1.0\n1248    0.0\n1249    0.0\n1250    0.0\n1251    0.0\n1252    0.0\n1253    0.0\n1254    1.0\n1255    1.0\n1256    0.0\n1257   -1.0\n1258    0.0\n1259    0.0\n1260    0.0\n1261    1.0\n1262    0.0\n1263    1.0\n1264    0.0\n1265    0.0\n1266    0.0\n1267    0.0\n1268   -1.0\n1269   -2.0\n1270    0.0\n1271    0.0\n1272    1.0\n1273    0.0\n1274    0.0\n1275   -1.0\n1276    2.0\n1277   -1.0\n1278   -1.0\n1279   -1.0\n1280    0.0\n1281    0.0\n1282    0.0\n1283   -1.0\n1284    1.0\n1285    1.0\n1286    0.0\n1287    1.0\n1288    0.0\n1289    0.0\n1290    1.0\n1291    0.0\n1292    0.0\n1293    1.0\n1294    0.0\n1295    0.0\n1296    0.0\n1297    0.0\n1298    0.0\n1299    2.0\n1300    0.0\n1301    0.0\n1302    0.0\n1303    1.0\n1304    0.0\n1305    0.0\n1306    0.0\n1307    2.0\n1308    0.0\n1309    0.0\n1310    0.0\n1311    0.0\n1312    0.0\n1313   -1.0\n1314   -1.0\n1315   -1.0\n1316    0.0\n1317    0.0\n1318   -1.0\n1319   -1.0\n1320    0.0\n1321    0.0\n1322    1.0\n1323   -1.0\n1324    0.0\n1325    0.0\n1326    0.0\n1327    0.0\n1328    0.0\n1329   -1.0\n1330   -1.0\n1331    0.0\n1332   -1.0\n1333    0.0\n1334    0.0\n1335    0.0\n1336    0.0\n1337    0.0\n1338    0.0\n1339    0.0\n1340    0.0\n1341    0.0\n1342    0.0\n1343    0.0\n1344    1.0\n1345    0.0\n1346    1.0\n1347    0.0\n1348    0.0\n1349    1.0\n1350    0.0\n1351    0.0\n1352    0.0\n1353    0.0\n1354    0.0\n1355    0.0\n1356    0.0\n1357    0.0\n1358    0.0\n1359    0.0\n1360    1.0\n1361    0.0\n1362    0.0\n1363    1.0\n1364    0.0\n1365    0.0\n1366    0.0\n1367   -1.0\n1368   -1.0\n1373    0.0\n1374    2.0\n1375    0.0\n1376    0.0\n1377    0.0\n1378   -1.0\n1379    0.0\n1380    0.0\n1381    0.0\n1382    0.0\n1383    0.0\n1384    0.0\n1385    0.0\n1386    0.0\n1387    0.0\n1388    0.0\n1389    0.0\n1390    0.0\n1391    1.0\n1392    0.0\n1393    0.0\n1394    0.0\n1396    0.0\n1397    0.0\n1398   -2.0\n1399    0.0\n1400    0.0\n1401    0.0\n1402    0.0\n1403   -2.0\n1404    0.0\n1405   -1.0\n1406    0.0\n1407    0.0\n1408   -1.0\n1409    0.0\n1410    0.0\n1411    0.0\n1412    0.0\n1413    0.0\n1414    1.0\n1415    0.0\n1416    1.0\n1417   -1.0\n1418    0.0\n1419    0.0\n1420    0.0\n1421    0.0\n1422    0.0\n1423    2.0\n1424    0.0\n1429    1.0\n1430    1.0\n1431   -1.0\n1432    0.0\n1433   -1.0\n1434   -1.0\n1435   -1.0\n1436    0.0\n1437    0.0\n1438    1.0\n1439    0.0\n1440   -1.0\n1441   -1.0\n1442    1.0\n1443    1.0\n1444    0.0\n1445   -1.0\n1446    1.0\n1448    1.0\n1449   -2.0\n1450   -1.0\n1451   -1.0\n1452   -1.0\n1453    0.0\n1454    0.0\n1455    0.0\n1456   -1.0\n1457    0.0\n1458    1.0\n1459   -1.0\n1460    0.0\n1461    1.0\n1462    0.0\n1463    0.0\n1464    0.0\n1465    0.0\n1466   -1.0\n1467    1.0\n1468   -1.0\n1469    2.0\n1470    0.0\n1471    1.0\n1472    0.0\n1473    1.0\n1474    0.0\n1475   -1.0\n1476    0.0\n1477   -1.0\n1478    2.0\n1479    1.0\n1480    1.0\n1481    1.0\n1482    1.0\n1483    1.0\n1484    2.0\n1485    0.0\n1486    0.0\n1487    1.0\n1488    1.0\n1489    0.0\n1490    0.0\n1491    1.0\n1492    1.0\n1493    0.0\n1494   -1.0\n1495    0.0\n1496    0.0\n1497    0.0\n1498   -1.0\n1499    0.0\n1500    0.0\n1501    0.0\n1502    0.0\n1503    0.0\n1504    0.0\n1505    2.0\n1506    0.0\n1507    0.0\n1508    0.0\n1509    1.0\n1510    0.0\n1511    0.0\n1512   -1.0\n1513   -1.0\n1514   -1.0\n1515   -1.0\n1516    1.0\n1517    0.0\n1518    1.0\n1519    0.0\n1520    0.0\n1521    1.0\n1522    1.0\n1523    1.0\n1524    0.0\n1525    0.0\n1526   -1.0\n1527    0.0\n1528    0.0\n1529    0.0\n1530    0.0\n1531    0.0\n1532    0.0\n1533    0.0\n1534   -1.0\n1535   -1.0\n1536    0.0\n1537    0.0\n1538    1.0\n1539    1.0\n1541   -1.0\n1542    0.0\n1543    0.0\n1544   -1.0\n1545   -1.0\n1546    1.0\n1547    1.0\n1548    1.0\n1549   -2.0\n1550    0.0\n1551    0.0\n1552    0.0\n1553    0.0\n1554    0.0\n1555   -1.0\n1556    0.0\n1557    0.0\n1558    0.0\n1559    0.0\n1560    0.0\n1561    0.0\n1562    0.0\n1563    0.0\n1564    0.0\n1565    0.0\n1566    0.0\n1567    0.0\n1568    0.0\n1569    0.0\n1570    0.0\n1571    0.0\n1572    0.0\n1573    0.0\n1574   -1.0\n1575    0.0\n1576    0.0\n1577    0.0\n1578    0.0\n1579    1.0\n1580    0.0\n1581    1.0\n1582    1.0\n1583    0.0\n1584    0.0\n1585    0.0\n1586    0.0\n1588    0.0\n1589    0.0\n1590    0.0\n1591    0.0\n1593    0.0\n1594    0.0\n1595    1.0\n1596    0.0\ndtype: float64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "#Show differences between calculated quality of the model and quality values of the dataset\n",
    "difference= df['Prediction'] - df['quality']\n",
    "dfn=(difference == -2).sum()\n",
    "dfp=(difference == 2).sum()\n",
    "difference_threshold=dfn+dfp\n",
    "\n",
    "#Print noises (threshold: difference bigger than 2)\n",
    "print(difference_threshold)\n",
    "display(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our assumption: The found noise data are feature noises --> jedes feature nach diesem Vorgehen bearbeiten? Oder die 58 einfach Instance löschen oder???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}